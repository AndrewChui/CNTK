{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using Pandas dataframes\n",
    "In this notebook we'll take a look at how to use pandas dataframes to train a neural network in CNTK.\n",
    "We're reusing the code from chapter 1 where we trained a iris classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "The model we use here is a basic classification model with a single hidden layer and an output layer that understands three possible species of flowers. We use a softmax activation function on the output layer and a sigmoid function on the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import default_options, input_variable\n",
    "from cntk.layers import Dense, Sequential\n",
    "from cntk.ops import log_softmax, relu, sigmoid\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(4, activation=sigmoid),\n",
    "    Dense(3, activation=log_softmax)\n",
    "])\n",
    "\n",
    "features = input_variable(4)\n",
    "labels = input_variable(3)\n",
    "\n",
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the training data\n",
    "We load up the iris dataset and preprocess it so that we end up with a set of floating point numbers as required by the model.\n",
    "Please note that we didn't split the dataset, this sample is just to demonstrate that you can use pandas dataframes. It is important to know that you can use `train_test_split` from the `scikit-learn` library to perform the necessary splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "df_source = pd.read_csv('iris.csv', \n",
    "    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n",
    "    index_col=False)\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "\n",
    "X = df_source.iloc[:, :4].values\n",
    "y = df_source.iloc[:, -1:].values\n",
    "\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "In the previous step we've converted our data from pandas to numpy arrays. This makes it fairly straightforward to train our model. We need a loss and learner to optimize the model. We've included a progress printer so we can see what the training process looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.1\n",
      "     1.84       1.84          0          0            16\n",
      "     1.51       1.34          0          0            48\n",
      "      1.5       1.49          0          0           112\n",
      "     1.43       1.43          0          0            16\n",
      "     1.25       1.16          0          0            48\n",
      "     1.31       1.35          0          0           112\n",
      "     1.38       1.38          0          0            16\n",
      "     1.22       1.14          0          0            48\n",
      "     1.26       1.29          0          0           112\n",
      "     1.38       1.38          0          0            16\n",
      "     1.22       1.14          0          0            48\n",
      "     1.25       1.27          0          0           112\n",
      "     1.38       1.38          0          0            16\n",
      "     1.22       1.14          0          0            48\n",
      "     1.24       1.25          0          0           112\n"
     ]
    }
   ],
   "source": [
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.learners import sgd \n",
    "from cntk.logging import ProgressPrinter\n",
    "\n",
    "progress_writer = ProgressPrinter(0)\n",
    "loss = cross_entropy_with_softmax(z, labels)\n",
    "learner = sgd(z.parameters, 0.1)\n",
    "\n",
    "train_summary = loss.train((X,y), parameter_learners=[learner], callbacks=[progress_writer], minibatch_size=16, max_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
