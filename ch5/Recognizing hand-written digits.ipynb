{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizing hand-written digits\n",
    "This notebook demonstrates how to use a convolutional neural network for image recognition.\n",
    "We're using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "You need a specially prepared dataset for this notebook. You can prepare the dataset for this notebook using the notebook \"Prepare the dataset.ipynb\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network structure\n",
    "The neural network has two convolution layers and two max-pooling layers mixed together.\n",
    "The output of the neural network is a softmax activated dense layer with 10 neurons. Each for one of the different labels that we can predict with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.layers import Convolution2D, Sequential, Dense, MaxPooling\n",
    "from cntk.ops import log_softmax, relu, sigmoid\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk import input_variable, default_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = input_variable((3,28,28))\n",
    "labels = input_variable(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with default_options(initialization=glorot_uniform, activation=relu):\n",
    "    model = Sequential([\n",
    "        Convolution2D(filter_shape=(5,5), strides=(1,1), num_filters=8, pad=True),\n",
    "        MaxPooling(filter_shape=(2,2), strides=(2,2)),\n",
    "        Convolution2D(filter_shape=(5,5), strides=(1,1), num_filters=16, pad=True),\n",
    "        MaxPooling(filter_shape=(3,3), strides=(3,3)),\n",
    "        Dense(10, activation=log_softmax)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for the model is a categorical cross entropy. We're using a `Function` object to combine the loss with a metric to measure the performance of the model. This `criterion_factory` is used to create the objective for the training logic. We're using a SGD learner for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import Function\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.metrics import classification_error\n",
    "\n",
    "@Function\n",
    "def criterion_factory(output, targets):\n",
    "    loss = cross_entropy_with_softmax(output, targets)\n",
    "    metric = classification_error(output, targets)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion_factory(z, labels)\n",
    "learner = sgd(z.parameters, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data source\n",
    "The data is stored as images on disk with a mapping file that combines the filename of the images with the label for each of the images. We're using random transforms during training to augment the training data in an attempt to improve performance.\n",
    "\n",
    "Note that these transforms are not applied during testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cntk.io import MinibatchSource, StreamDef, StreamDefs, ImageDeserializer, INFINITELY_REPEAT\n",
    "import cntk.io.transforms as xforms\n",
    "\n",
    "def create_datasource(folder, train=True, max_sweeps=INFINITELY_REPEAT):\n",
    "    mapping_file = os.path.join(folder, 'mapping.bin')\n",
    "    \n",
    "    image_transforms = []\n",
    "    \n",
    "    if train:\n",
    "        image_transforms += [\n",
    "            xforms.crop(crop_type='randomside', side_ratio=0.8)\n",
    "        ]\n",
    "        \n",
    "    # Always scale the image, the cropping breaks them during training.\n",
    "    # The scaling fixes the problem.\n",
    "    image_transforms += [\n",
    "        xforms.scale(width=28, height=28, channels=3, interpolations='linear')\n",
    "    ]\n",
    "    \n",
    "    stream_definitions = StreamDefs(\n",
    "        features=StreamDef(field='image', transforms=image_transforms),\n",
    "        labels=StreamDef(field='label', shape=10)\n",
    "    )\n",
    "    \n",
    "    deserializer = ImageDeserializer(mapping_file, stream_definitions)\n",
    "    \n",
    "    return MinibatchSource(deserializer, max_sweeps=max_sweeps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data and testing data is stored in separate folders.\n",
    "You need a separate data source for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasource = create_datasource('mnist_train')\n",
    "test_datasource = create_datasource('mnist_test', max_sweeps=1, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The model is trained for one epoch with a batchsize of 64. We've added the progress printer to visualize the output of the training session. We've also included the test set here to validate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.2\n",
      "      105        105      0.938      0.938            64\n",
      " 1.01e+07   1.51e+07      0.901      0.883           192\n",
      " 4.31e+06          2      0.897      0.895           448\n",
      " 2.01e+06          2      0.902      0.906           960\n",
      " 9.73e+05          2      0.897      0.893          1984\n",
      " 4.79e+05          2      0.894      0.891          4032\n",
      " 2.38e+05          2      0.891      0.889          8128\n",
      " 1.18e+05          2       0.89      0.889         16320\n",
      "  5.9e+04          2       0.89       0.89         32704\n",
      "Finished Evaluation [1]: Minibatch[1-313]: metric = 88.65% * 10000;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'updates': [{'loss': 105.38042449951172, 'metric': 0.9375, 'samples': 64},\n",
       "  {'loss': 15084627.30978775, 'metric': 0.8828125, 'samples': 128},\n",
       "  {'loss': 2.0, 'metric': 0.89453125, 'samples': 256},\n",
       "  {'loss': 2.0, 'metric': 0.90625, 'samples': 512},\n",
       "  {'loss': 2.0, 'metric': 0.892578125, 'samples': 1024},\n",
       "  {'loss': 2.0, 'metric': 0.890625, 'samples': 2048},\n",
       "  {'loss': 2.0, 'metric': 0.88916015625, 'samples': 4096},\n",
       "  {'loss': 2.0, 'metric': 0.8887939453125, 'samples': 8192},\n",
       "  {'loss': 2.0, 'metric': 0.88958740234375, 'samples': 16384}],\n",
       " 'epoch_summaries': [{'loss': 32182.645333333334,\n",
       "   'metric': 0.8897833333333334,\n",
       "   'samples': 60000}],\n",
       " 'test_summary': {'metric': 0.8865, 'samples': 10000}}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.train import TestConfig\n",
    "\n",
    "progress_writer = ProgressPrinter(0)\n",
    "test_config = TestConfig(test_datasource)\n",
    "\n",
    "input_map = {\n",
    "    features: train_datasource.streams.features,\n",
    "    labels: train_datasource.streams.labels\n",
    "}\n",
    "\n",
    "loss.train(train_datasource, \n",
    "           max_epochs=1,\n",
    "           minibatch_size=64,\n",
    "           epoch_size=60000, \n",
    "           parameter_learners=[learner], \n",
    "           model_inputs_to_streams=input_map,  \n",
    "           callbacks=[progress_writer, test_config])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
