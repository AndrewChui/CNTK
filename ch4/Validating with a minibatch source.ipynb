{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding validation to models trained with a minibatch source\n",
    "In this notebook we'll demonstrate how to add metrics to the training process when you're using minibatch sources. First we'll set up the data source and the model. We then train it using a loss and include a metric for validation. Finally we can use the metric to validate the model at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data source\n",
    "The data source for the model is a CTF file containing the training data for the model.\n",
    "We've split the data file into a training set and a test set. We've created a utility function to\n",
    "turn a file into a minibatch source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.io import StreamDef, StreamDefs, MinibatchSource, CTFDeserializer, INFINITELY_REPEAT\n",
    "\n",
    "def create_datasource(filename, limit=INFINITELY_REPEAT):\n",
    "\n",
    "    labels_stream = StreamDef(field='labels', shape=3, is_sparse=False)\n",
    "    features_stream = StreamDef(field='features', shape=4, is_sparse=False)\n",
    "\n",
    "    deserializer = CTFDeserializer(filename, StreamDefs(labels=labels_stream, features=features_stream))\n",
    "\n",
    "    minibatch_source = MinibatchSource(deserializer, randomize=True, max_sweeps=limit)\n",
    "    \n",
    "    return minibatch_source\n",
    "\n",
    "training_source = create_datasource('iris_train.ctf')\n",
    "test_source = create_datasource('iris_test.ctf', limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the helper function we can now create multiple minibatch sources. One for testing and one for training.\n",
    "The training data source can be iterated over an unlimited number of times. This is required to be able to run multiple epochs of training. The test data source has a limited number of sweeps, because we only want to pass the test data through the model once at the end of the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "The model we're using is a classification model that is capable of classifying iris flowers of three different species. The model has four input neurons and three output neurons corresponding to the number of features in the dataset and the number of species it can classify. It features a single hidden layer of 4 neurons as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import default_options, input_variable\n",
    "from cntk.layers import Dense, Sequential\n",
    "from cntk.ops import log_softmax, relu, sigmoid\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(4, activation=sigmoid),\n",
    "    Dense(3, activation=log_softmax)\n",
    "])\n",
    "\n",
    "features = input_variable(4)\n",
    "labels = input_variable(3)\n",
    "\n",
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We're going to train the model using a cross entropy loss and validate it using the f-measure metric that we've seen before in chapter 4 of the book. We're using the SGD learner to optimize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.losses import cross_entropy_with_softmax, fmeasure\n",
    "from cntk.learners import sgd \n",
    "\n",
    "loss = cross_entropy_with_softmax(z, labels)\n",
    "metric = fmeasure(z, labels, beta=1)\n",
    "learner = sgd(z.parameters, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the model against the test set we need to create a test configuration.\n",
    "This configuration tells the training session how to run a test run at the end of the training session.\n",
    "\n",
    "The training configuration needs a minibatch source that allows a limited number of sweeps. This is needed to prevent the training session from running forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.train import TestConfig\n",
    "\n",
    "test_config = TestConfig(test_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training logic we'll use the `training_session` function from CNTK. This function can be set up with a training minibatch source, parameters to control how the data is fed into the model and how much data is used per minibatch. We can add to this another keyword argument `test_config` which tells the session how to run a test at the end of the session.\n",
    "\n",
    "Once we have the session configured we can call train on it to start the training process.\n",
    "When the training process completes the test config is used automatically to validate the model performance for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.1\n",
      "     1.57       1.57      0.214      0.214            16\n",
      "     1.38       1.28      0.264      0.289            48\n",
      "     1.41       1.44      0.147     0.0589           112\n",
      "     1.27       1.15     0.0988     0.0568           240\n",
      "     1.17       1.08     0.0807     0.0638           496\n",
      "      1.1       1.03     0.0949      0.109          1008\n",
      "    0.973      0.845      0.206      0.315          2032\n",
      "    0.781       0.59      0.409       0.61          4080\n",
      "Finished Evaluation [1]: Minibatch[1-1]: metric = 70.72% * 30;\n"
     ]
    }
   ],
   "source": [
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.train import Trainer, training_session\n",
    "\n",
    "minibatch_size = 16\n",
    "samples_per_epoch = 150\n",
    "num_epochs = 30\n",
    "max_samples = samples_per_epoch * num_epochs\n",
    "\n",
    "input_map = {\n",
    "    features: training_source.streams.features,\n",
    "    labels: training_source.streams.labels\n",
    "}\n",
    "\n",
    "progress_writer = ProgressPrinter(0)\n",
    "trainer = Trainer(z, (loss, metric), learner, progress_writer)\n",
    "\n",
    "session = training_session(trainer, \n",
    "                           mb_source=training_source,\n",
    "                           mb_size=minibatch_size, \n",
    "                           model_inputs_to_streams=input_map, \n",
    "                           max_samples=max_samples,\n",
    "                           test_config=test_config)\n",
    "\n",
    "session.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
