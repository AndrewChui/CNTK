{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tensorboard\n",
    "This notebook demonstrates how to log data to tensorboard during training and how to use tensorboard to visualize the model. Check out chapter 4 of the book to learn more on how to use tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "We're using the iris flower classification model from previous chapters. \n",
    "The model has 4 input neurons and 3 output neurons. We'll be using a softmax activation on the output layer to create a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import default_options, input_variable\n",
    "from cntk.layers import Dense, Sequential\n",
    "from cntk.ops import log_softmax, relu, sigmoid\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(4, activation=relu),\n",
    "    Dense(3, activation=log_softmax)\n",
    "])\n",
    "\n",
    "features = input_variable(4)\n",
    "labels = input_variable(3)\n",
    "\n",
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "We're going to use the original CSV file for the iris dataset.\n",
    "It contains four columns with features. The final column is the label (the species of the sample).\n",
    "\n",
    "The labels are stored as string and we need to have a binary encoded variant.\n",
    "So we'll use a Binarizer from scikit-learn to encode the labels in the proper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "df_source = pd.read_csv('iris.csv', \n",
    "    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n",
    "    index_col=False)\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "\n",
    "X = df_source.iloc[:, :4].values\n",
    "y = df_source.iloc[:, -1:].values\n",
    "\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into a training and test set\n",
    "Before we start training, we'll split the dataset into a training and test set using the `train_test_split` function from scikit-learn. This gives us a small set of samples to later validate the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Now that we have a dataset, let's train the model with it.\n",
    "For this we define a loss and a learner. We can then use the loss to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk\n",
    "\n",
    "@cntk.Function\n",
    "def criterion_factory(output, target):\n",
    "    loss = cntk.losses.cross_entropy_with_softmax(output, target)\n",
    "    metric = cntk.losses.fmeasure(output, target, beta=1)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've included a tensorboard progress writer in the trainer callbacks so that metrics are logged in the native tensorboard format. You can view the data by running the command: `tensorboard --logdir logs` in the directory that this notebook is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting log to file test_log.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from cntk import DataUnit\n",
    "from cntk.train.training_session import CrossValidationConfig\n",
    "from cntk.losses import cross_entropy_with_softmax\n",
    "from cntk.learners import sgd \n",
    "from cntk.logging import ProgressPrinter, TensorBoardProgressWriter\n",
    "\n",
    "loss = criterion_factory(z, labels)\n",
    "learner = sgd(z.parameters, 0.1)\n",
    "\n",
    "train_callbacks = [\n",
    "    ProgressPrinter(freq=0, log_to_file='test_log.txt'),\n",
    "    TensorBoardProgressWriter(log_dir='logs/{}'.format(time.time()), freq=1, model=z)\n",
    "]\n",
    "\n",
    "train_summary = loss.train((X_train,y_train), \n",
    "                           parameter_learners=[learner], \n",
    "                           callbacks=train_callbacks, \n",
    "                           minibatch_size=16, max_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the tensorboard connection after you're done. This ensures that all logging data is flushed to disk. Otherwise you may miss data  in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks[1].close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
