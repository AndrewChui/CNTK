{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a manual minibatch loop\n",
    "In this notebook we'll retrain our flower classification model using a manual minibatch loop.\n",
    "The model is the same as before, 4 input features and a binary encoded label as output. \n",
    "\n",
    "We're going to pretent that de dataset is too big to fit in memory. So we'll load it in chunks. Since our data is stored as CSV we can still use pandas but with a different configuration. The LabelBinarizer that we've used in previous samples no longer works as you can't train that component in chunks. Instead we'll use a different technique to encode the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model is a classification neural network with 4 inputs and 3 outputs. The 4 inputs correspond with the number of input features that we have in our dataset. The 3 outputs represent a binary encoding of 3 possible species of flowers that we can classify.\n",
    "\n",
    "The loss function for the model is a categorical cross entropy function because we're dealing with a multi-class classification problem. The learner is a standard SGD (Stochastic Gradient Descent) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import default_options, input_variable\n",
    "from cntk.layers import Dense, Sequential\n",
    "from cntk.ops import log_softmax, relu, sigmoid\n",
    "from cntk.learners import sgd \n",
    "\n",
    "model = Sequential([\n",
    "    Dense(4, activation=sigmoid),\n",
    "    Dense(3, activation=log_softmax)\n",
    "])\n",
    "\n",
    "features = input_variable(4)\n",
    "labels = input_variable(3)\n",
    "\n",
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for the model is defined as a combination of a loss and a metric. We use the criterion_factory utility to create this as a CNTK function object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk\n",
    "from cntk.losses import cross_entropy_with_softmax, fmeasure\n",
    "\n",
    "@cntk.Function\n",
    "def criterion_factory(outputs, targets):\n",
    "    loss = cross_entropy_with_softmax(outputs, targets)\n",
    "    metric = fmeasure(outputs, targets, beta=1)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion_factory(z, labels)\n",
    "learner = sgd(z.parameters, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the labels\n",
    "We still have a set of labels  in our dataset so we need to encode to a binary representation. Sadly sklearn requires us to load the whole dataset into memory if we want to train a LabelBinarizer for this purpose like we did before in previous samples. So instead of using a LabelBinarizer we create a manual mapping between the labels and their encoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "This next section implements a single epoch of training using a manual minibatch loop.\n",
    "You can wrap the code after the creation of the trainer in an extra for-loop to introduce multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.1\n",
      "     1.45       1.45     -0.189     -0.189            16\n",
      "     1.24       1.13    -0.0382     0.0371            48\n",
      "     1.13       1.04      0.141      0.276           112\n",
      "     1.21        1.3     0.0382    -0.0599           230\n",
      "      1.2       1.18      0.037     0.0358           466"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input2238\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     1.17       1.13     0.0524     0.0674           948\n",
      "     1.08       0.99      0.129      0.204          1912\n",
      "    0.865      0.654      0.343      0.557          3830\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.train import Trainer\n",
    "\n",
    "progress_writer = ProgressPrinter(0)\n",
    "trainer = Trainer(z, loss, learner, progress_writer)\n",
    "\n",
    "for _ in range(0,30):\n",
    "    input_data = pd.read_csv('iris.csv', \n",
    "        names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n",
    "        index_col=False, chunksize=16)\n",
    "\n",
    "    for df_batch in input_data:\n",
    "        feature_values = df_batch.iloc[:,:4].values\n",
    "        feature_values = feature_values.astype(np.float32)\n",
    "\n",
    "        label_values = df_batch.iloc[:,-1]\n",
    "\n",
    "        label_values = label_values.map(lambda x: label_mapping[x])\n",
    "        label_values = label_values.values\n",
    "\n",
    "        encoded_labels = np.zeros((label_values.shape[0], 3))\n",
    "        encoded_labels[np.arange(label_values.shape[0]), label_values] = 1.\n",
    "\n",
    "        trainer.train_minibatch({features: feature_values, labels: encoded_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-11]: metric = 65.71% * 166;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\cntk\\core.py:82: RuntimeWarning: data is not C contiguous; rearrange your data/computation to avoid costly data conversions\n",
      "  RuntimeWarning)\n",
      "c:\\anaconda3\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input2238\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    }
   ],
   "source": [
    "from cntk import Evaluator\n",
    "\n",
    "evaluator = Evaluator(loss.outputs[1], [progress_writer])\n",
    "input_data = pd.read_csv('iris.csv', \n",
    "        names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n",
    "        index_col=False, chunksize=16)\n",
    "\n",
    "for df_batch in input_data:\n",
    "    feature_values = df_batch.iloc[:,:4].values\n",
    "    feature_values = feature_values.astype(np.float32)\n",
    "\n",
    "    label_values = df_batch.iloc[:,-1]\n",
    "    \n",
    "    label_values = label_values.map(lambda x: label_mapping[x])\n",
    "    label_values = label_values.values\n",
    "    \n",
    "    encoded_labels = np.zeros((label_values.shape[0], 3))\n",
    "    encoded_labels[np.arange(label_values.shape[0]), label_values] = 1.\n",
    "    \n",
    "    evaluator.test_minibatch({ features: feature_values, labels: encoded_labels})\n",
    "    \n",
    "evaluator.summarize_test_progress()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
