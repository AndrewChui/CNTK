{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing trained models\n",
    "This notebook contains the same sample as we used in Chapter 6, Working with timeseries data. It trains a recurrent neural network containing LSTM recurrent units on solarpanel data. This notebook is an extended version of the original one with checkpointing. We've left the original comments in for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.losses import squared_error\n",
    "from cntk.io import CTFDeserializer, MinibatchSource, INFINITELY_REPEAT, StreamDefs, StreamDef\n",
    "from cntk.learners import adam\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.train import TestConfig, CheckpointConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook uses a set of constants to control various settings.\n",
    "The most important settings are the batch size, epoch size and number of epochs to train for.\n",
    "\n",
    "We've normalized the training data based on the maximum total power generated by the solar panel. \n",
    "This value is stored as a constant here to denormalize the output of the neural network normal usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 14 * 10\n",
    "EPOCH_SIZE = 12434\n",
    "EPOCHS = 10\n",
    "\n",
    "# This value is required to convert the normalized values back to their original value.\n",
    "# You can obtain this value by looking at the maximum value for the solar.total column\n",
    "NORMALIZE = 19100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "The model we're using is a recurrent neural network with an LSTM as the implementation for the recurrent layer in the network. We've wrapped the LSTM in a Fold layer because we're only interested in the final output of the recurrent layer. \n",
    "The output of the network is generated using a final Dense layer.\n",
    "\n",
    "Note, the input features for the model are stored in a sequence input variable. This is required since we're working with sequences rather than single samples. The target output is stored in a regular input variable as we're only interested in predicting a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import sequence, default_options, input_variable\n",
    "from cntk.layers import Recurrence, LSTM, Dropout, Dense, Sequential, Fold\n",
    "\n",
    "features = sequence.input_variable(1)\n",
    "\n",
    "with default_options(initial_state = 0.1):\n",
    "    model = Sequential([\n",
    "        Fold(LSTM(15)),\n",
    "        Dense(1)\n",
    "    ])(features)\n",
    "    \n",
    "target = input_variable(1, dynamic_axes=model.dynamic_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The model is trained using a mean squared error loss function. The data for the model is coming from a set of CTF Files containing sequences of measurements per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import Function\n",
    "\n",
    "@Function\n",
    "def criterion_factory(z, t):\n",
    "    loss = squared_error(z, t)\n",
    "    metric = squared_error(z, t)    \n",
    "    \n",
    "    return loss, metric\n",
    "\n",
    "loss = criterion_factory(model, target)\n",
    "learner = adam(model.parameters, lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load data into the training process we need to deserialize sequences from a set of CTF files. The `create_datasource` function is a useful utility function to create both the training and test datasources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasource(filename, sweeps=INFINITELY_REPEAT):\n",
    "    target_stream = StreamDef(field='target', shape=1, is_sparse=False)\n",
    "    features_stream = StreamDef(field='features', shape=1, is_sparse=False)\n",
    "\n",
    "    deserializer = CTFDeserializer(filename, StreamDefs(features=features_stream, target=target_stream))\n",
    "    datasource = MinibatchSource(deserializer, randomize=True, max_sweeps=sweeps)    \n",
    "    \n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasource = create_datasource('solar_train.ctf')\n",
    "test_datasource = create_datasource('solar_val.ctf', sweeps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've setup the data sources, model, and loss function let's start the training process.\n",
    "Please be aware, this takes a long time on a computer with just a CPU. If you can, use a GPU to train this model.\n",
    "\n",
    "**Note** Checkpointing is enabled in this version of the notebook, so you can interrupt training at any point. When you restart the notebook at a later time you can execute all cells again and it will continue training where it left of the last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Finished Evaluation [1]: Minibatch[1-598]: metric = 0.86% * 2239;\n"
     ]
    }
   ],
   "source": [
    "progress_writer = ProgressPrinter(0)\n",
    "test_config = TestConfig(test_datasource)\n",
    "\n",
    "input_map = {\n",
    "    features: train_datasource.streams.features,\n",
    "    target: train_datasource.streams.target\n",
    "}\n",
    "\n",
    "checkpoint_config = CheckpointConfig('solar.dnn', frequency=100, restore=True, preserve_all=False)\n",
    "\n",
    "history = loss.train(\n",
    "    train_datasource, \n",
    "    epoch_size=EPOCH_SIZE,\n",
    "    parameter_learners=[learner], \n",
    "    model_inputs_to_streams=input_map,\n",
    "    callbacks=[progress_writer, test_config, checkpoint_config],\n",
    "    minibatch_size=BATCH_SIZE,\n",
    "    max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the model in ONNX format\n",
    "When you're done training you can store the model on disk in the ONNX format so you can use it from, for example, C# or Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import ModelFormat\n",
    "\n",
    "model.save('solar.onnx', format=ModelFormat.ONNX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
