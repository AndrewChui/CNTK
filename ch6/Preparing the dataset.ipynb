{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "This notebook describes how to create a timeseries dataset for use with the CNTK minibatch source. \n",
    "The dataset for this sample is a [free open-source dataset](https://www.cntk.ai/jup/dat/solar.csv) containing measurements of a set of solar panels during the day. The data is stored as a CSV file on disk, so we can use pandas to process it.\n",
    "\n",
    "The output of this notebook is a CTF file containing sequences of varying length used to train a recurrent neural network to predict total solar power output for a set of solar panels. We'll produce two datasets: A training set and a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The dataset is stored as a table not a set of sequences. First we'll need to load the data and normalize it so we have proper input to generate sequences from. The dataset has a timestamp we can use as the index. This makes it easier to group the data per day so we can generate sequences for a specific day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19100.0\n"
     ]
    }
   ],
   "source": [
    "df_solar = pd.read_csv('solar.csv', index_col='time', parse_dates=['time'])\n",
    "\n",
    "df_solar['date'] = df_solar.index.date\n",
    "\n",
    "print(df_solar['solar.total'].max())\n",
    "\n",
    "# Normalize the data so all values are between 0 and 1.\n",
    "# This is required, because we are using sigmoid and tanh activations in our model.\n",
    "# These kind of activations don't work for values that are not within the 0 to 1 range.\n",
    "df_solar['solar.current'] /= df_solar['solar.total'].max()\n",
    "df_solar['solar.total'] /= df_solar['solar.total'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the code above is that we now have a dataset that has an index containing the timestamps for the measurements. The dataset contains normalized values for the current output and the total output for a day. We can now start to group up measurements per day and calculate the total power generated for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_solar.groupby(df_solar.index.date).max()\n",
    "df_grouped.columns = ['solar.current.max', 'solar.total.max', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grouped dataset contains the total power generated for a particular day `solar.total.max`. It also contains the maximum power generated in 30 minutes for that day which is stored in `solar.current.max`. We can now merge both datasets to get a dataset that contains the original sequences, but with the totals for each day added to each entry of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_solar, df_grouped, right_index=True, on='date')\n",
    "df_merged = df_merged[['solar.current', 'solar.total', 'solar.current.max','solar.total.max']]\n",
    "\n",
    "df_per_day = df_merged.groupby(df_merged.index.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "The data is stored as a table but we need to get sequences into a CTF file. We're going to have to create sequences from the original dataset. Each day is its own sequence that we can use to predict the total power generated for a day.\n",
    "\n",
    "There are a few things that we have to keep in mind to ensure that our model does sensible things:\n",
    "\n",
    " * Each day that has less than 8 measurements is considered faulty and discarded.\n",
    " * Each day that has more than 14 measurements is truncated to 14 measurements.\n",
    " \n",
    "We'll create two lists of datapoints, one with the targets for each day and another one that contains the sequence of datapoints for that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "sequences = []\n",
    "\n",
    "for _, group in df_per_day:\n",
    "    # Less then 8 measurements on a day is considered invalid.\n",
    "    if len(group['solar.total'].values) < 8:\n",
    "        continue\n",
    "        \n",
    "    day_total = group['solar.total.max'].values[0]\n",
    "    sequence = group[['solar.total']].values[0:14, :] \n",
    "    \n",
    "    for j in range(2, len(sequence)):\n",
    "        derived_seq = sequence[:j]\n",
    "        sequences.append(derived_seq)\n",
    "        targets.append(day_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the data\n",
    "Once we have the data preprocessed into sequences and targets, let's create the CTF file. The CTF file format says that you can store a sequence over multiple lines, so each sample from the sequence goes on a separate line to make things simple.\n",
    "This looks like this:\n",
    "\n",
    "```\n",
    "0 |target 0.5392670157068062 |features 8.848167838850571e-05\n",
    "0 |features 0.000594764392413394\n",
    "1 |target 0.5392670157068062 |features 8.848167838850571e-05\n",
    "1 |features 0.000594764392413394\n",
    "1 |features 0.0035340314136125656\n",
    "2 |target 0.5392670157068062 |features 8.848167838850571e-05\n",
    "2 |features 0.000594764392413394\n",
    "2 |features 0.0035340314136125656\n",
    "2 |features 0.013115183246073298\n",
    "```\n",
    "The first line of a new sequence includes the target for that sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly train the model we need to have two datasets, a training set and a validation set.\n",
    "We're splitting the whole dataset in three chunks:\n",
    "\n",
    " 1. A training set containing 70% of all the data.\n",
    " 2. A validation set containing 20% of all the data.\n",
    " 3. A test set containing 10% of all the data.\n",
    " \n",
    "We'll store the first two sets in a CTF file format for use with a minibatch source later on. We're going to store the test set as a pickle file with numpy arrays. This makes it easier to load the test samples in a ready-to-go format for making predictions with our model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences,targets, test_size=0.1)\n",
    "train_x, val_x, train_y, val_y = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dataset(filename, x, y):\n",
    "    with open(filename, 'w') as output_file:\n",
    "        for i in range(0, len(y)):\n",
    "            sequence = x[i]\n",
    "            target = y[i]\n",
    "\n",
    "            for j,element in enumerate(sequence):\n",
    "                output_file.write('{} '.format(i))\n",
    "\n",
    "                if j == 0:\n",
    "                    output_file.write('|target {} '.format(target))\n",
    "\n",
    "                output_file.write('|features {}\\n'.format(element[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dataset('solar_train.ctf', train_x, train_y)\n",
    "store_dataset('solar_val.ctf', val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test_items = []\n",
    "\n",
    "for item in X_test:\n",
    "    test_items.append(np.array(item))\n",
    "    \n",
    "with open('test_samples.pkl', 'wb') as test_file:\n",
    "    pickle.dump(test_items, test_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
