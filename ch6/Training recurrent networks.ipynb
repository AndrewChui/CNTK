{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training recurrent neural networks\n",
    "This notebook contains the sample code to train a recurrent neural networks to predict the total power output for a day of a solar panel. The dataset is preprocessed and available with this notebook. You can however regenerate the dataset using the notebook \"Prepare the dataset.ipynb\" which is in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk.losses import squared_error\n",
    "from cntk.io import CTFDeserializer, MinibatchSource, INFINITELY_REPEAT, StreamDefs, StreamDef\n",
    "from cntk.learners import adam\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.train import TestConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook uses a set of constants to control various settings.\n",
    "The most important settings are the batch size, epoch size and number of epochs to train for.\n",
    "\n",
    "We've normalized the training data based on the maximum total power generated by the solar panel. \n",
    "This value is stored as a constant here to denormalize the output of the neural network normal usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 14 * 10\n",
    "EPOCH_SIZE = 12434\n",
    "EPOCHS = 10\n",
    "\n",
    "# This value is required to convert the normalized values back to their original value.\n",
    "# You can obtain this value by looking at the maximum value for the solar.total column\n",
    "NORMALIZE = 19100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "The model we're using is a recurrent neural network with an LSTM as the implementation for the recurrent layer in the network. We've wrapped the LSTM in a Fold layer because we're only interested in the final output of the recurrent layer. \n",
    "The output of the network is generated using a final Dense layer.\n",
    "\n",
    "Note, the input features for the model are stored in a sequence input variable. This is required since we're working with sequences rather than single samples. The target output is stored in a regular input variable as we're only interested in predicting a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import sequence, default_options, input_variable\n",
    "from cntk.layers import Recurrence, LSTM, Dropout, Dense, Sequential, Fold\n",
    "\n",
    "features = sequence.input_variable(1)\n",
    "\n",
    "with default_options(initial_state = 0.1):\n",
    "    model = Sequential([\n",
    "        Fold(LSTM(15)),\n",
    "        Dense(1)\n",
    "    ])(features)\n",
    "    \n",
    "target = input_variable(1, dynamic_axes=model.dynamic_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The model is trained using a mean squared error loss function. The data for the model is coming from a set of CTF Files containing sequences of measurements per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cntk import Function\n",
    "\n",
    "@Function\n",
    "def criterion_factory(z, t):\n",
    "    loss = squared_error(z, t)\n",
    "    metric = squared_error(z, t)    \n",
    "    \n",
    "    return loss, metric\n",
    "\n",
    "loss = criterion_factory(model, target)\n",
    "learner = adam(model.parameters, lr=0.005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load data into the training process we need to deserialize sequences from a set of CTF files. The `create_datasource` function is a useful utility function to create both the training and test datasources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasource(filename, sweeps=INFINITELY_REPEAT):\n",
    "    target_stream = StreamDef(field='target', shape=1, is_sparse=False)\n",
    "    features_stream = StreamDef(field='features', shape=1, is_sparse=False)\n",
    "\n",
    "    deserializer = CTFDeserializer(filename, StreamDefs(features=features_stream, target=target_stream))\n",
    "    datasource = MinibatchSource(deserializer, randomize=True, max_sweeps=sweeps)    \n",
    "    \n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasource = create_datasource('solar_train.ctf')\n",
    "test_datasource = create_datasource('solar_val.ctf', sweeps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've setup the data sources, model, and loss function let's start the training process.\n",
    "Please be aware, this takes a long time on a computer with just a CPU. If you can, use a GPU to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.005\n",
      "     0.66       0.66       0.66       0.66            19\n",
      "    0.637      0.626      0.637      0.626            59\n",
      "    0.699      0.752      0.699      0.752           129\n",
      "    0.676      0.656      0.676      0.656           275\n",
      "    0.622      0.573      0.622      0.573           580\n",
      "    0.577      0.531      0.577      0.531          1150\n",
      "    0.506      0.436      0.506      0.436          2298\n",
      "    0.363      0.223      0.363      0.223          4643\n",
      "      0.2     0.0391        0.2     0.0391          9328\n",
      "   0.0168     0.0168     0.0168     0.0168            20\n",
      "   0.0262      0.032     0.0262      0.032            52\n",
      "   0.0254     0.0248     0.0254     0.0248           122\n",
      "   0.0253     0.0251     0.0253     0.0251           268\n",
      "    0.025     0.0248      0.025     0.0248           552\n",
      "   0.0226     0.0203     0.0226     0.0203          1132\n",
      "   0.0224     0.0223     0.0224     0.0223          2284\n",
      "   0.0215     0.0207     0.0215     0.0207          4626\n",
      "   0.0201     0.0187     0.0201     0.0187          9320\n",
      "   0.0124     0.0124     0.0124     0.0124            21\n",
      "   0.0135     0.0142     0.0135     0.0142            56\n",
      "   0.0149     0.0159     0.0149     0.0159           131\n",
      "   0.0157     0.0165     0.0157     0.0165           286\n",
      "   0.0164      0.017     0.0164      0.017           587\n",
      "   0.0154     0.0144     0.0154     0.0144          1153\n",
      "   0.0143     0.0132     0.0143     0.0132          2310\n",
      "   0.0137     0.0131     0.0137     0.0131          4631\n",
      "   0.0127     0.0118     0.0127     0.0118          9294\n",
      "  0.00555    0.00555    0.00555    0.00555            22\n",
      "    0.007     0.0078      0.007     0.0078            62\n",
      "  0.00917     0.0109    0.00917     0.0109           139\n",
      "   0.0117      0.014     0.0117      0.014           295\n",
      "   0.0112     0.0107     0.0112     0.0107           576\n",
      "   0.0108     0.0104     0.0108     0.0104          1169\n",
      "   0.0105     0.0102     0.0105     0.0102          2329\n",
      "   0.0103     0.0101     0.0103     0.0101          4670\n",
      "   0.0102     0.0101     0.0102     0.0101          9340\n",
      "  0.00392    0.00392    0.00392    0.00392            18\n",
      "  0.00736    0.00908    0.00736    0.00908            54\n",
      "  0.00736    0.00735    0.00736    0.00735           132\n",
      "  0.00915     0.0108    0.00915     0.0108           278\n",
      "  0.00971     0.0102    0.00971     0.0102           563\n",
      "  0.00966    0.00962    0.00966    0.00962          1150\n",
      "  0.00998     0.0103    0.00998     0.0103          2333\n",
      "  0.00954    0.00911    0.00954    0.00911          4679\n",
      "  0.00971    0.00987    0.00971    0.00987          9395\n",
      "   0.0186     0.0186     0.0186     0.0186            18\n",
      "   0.0113     0.0074     0.0113     0.0074            52\n",
      "   0.0113     0.0112     0.0113     0.0112           131\n",
      "  0.00989    0.00854    0.00989    0.00854           264\n",
      "  0.00951    0.00919    0.00951    0.00919           568\n",
      "  0.00987     0.0102    0.00987     0.0102          1125\n",
      "  0.00958    0.00931    0.00958    0.00931          2303\n",
      "  0.00966    0.00974    0.00966    0.00974          4653\n",
      "  0.00965    0.00963    0.00965    0.00963          9343\n",
      "  0.00769    0.00769    0.00769    0.00769            21\n",
      "  0.00575    0.00456    0.00575    0.00456            55\n",
      "  0.00883     0.0112    0.00883     0.0112           128\n",
      "   0.0101     0.0111     0.0101     0.0111           284\n",
      "  0.00994    0.00982    0.00994    0.00982           574\n",
      "  0.00921    0.00851    0.00921    0.00851          1173\n",
      "  0.00965     0.0101    0.00965     0.0101          2354\n",
      "  0.00951    0.00937    0.00951    0.00937          4688\n",
      "  0.00953    0.00954    0.00953    0.00954          9321\n",
      "  0.00805    0.00805    0.00805    0.00805            19\n",
      "  0.00765    0.00742    0.00765    0.00742            54\n",
      "  0.00985     0.0115    0.00985     0.0115           125\n",
      "  0.00891    0.00803    0.00891    0.00803           258\n",
      "  0.00903    0.00914    0.00903    0.00914           538\n",
      "  0.00907     0.0091    0.00907     0.0091          1123\n",
      "  0.00899    0.00891    0.00899    0.00891          2294\n",
      "  0.00946    0.00992    0.00946    0.00992          4651\n",
      "  0.00923      0.009    0.00923      0.009          9337\n",
      "  0.00752    0.00752    0.00752    0.00752            20\n",
      "  0.00932     0.0103    0.00932     0.0103            55\n",
      "     0.01     0.0105       0.01     0.0105           130\n",
      "     0.01       0.01       0.01       0.01           263\n",
      "   0.0103     0.0106     0.0103     0.0106           561\n",
      "  0.00914    0.00803    0.00914    0.00803          1159\n",
      "  0.00893    0.00871    0.00893    0.00871          2327\n",
      "  0.00904    0.00915    0.00904    0.00915          4642\n",
      "  0.00926    0.00948    0.00926    0.00948          9313\n",
      "   0.0066     0.0066     0.0066     0.0066            17\n",
      "  0.00788    0.00844    0.00788    0.00844            56\n",
      "  0.00907    0.00995    0.00907    0.00995           132\n",
      "  0.00896    0.00886    0.00896    0.00886           285\n",
      "  0.00984     0.0107    0.00984     0.0107           581\n",
      "   0.0095    0.00916     0.0095    0.00916          1176\n",
      "  0.00919    0.00888    0.00919    0.00888          2338\n",
      "  0.00898    0.00877    0.00898    0.00877          4697\n",
      "  0.00901    0.00905    0.00901    0.00905          9381\n",
      "Finished Evaluation [1]: Minibatch[1-598]: metric = 0.88% * 2239;\n"
     ]
    }
   ],
   "source": [
    "progress_writer = ProgressPrinter(0)\n",
    "test_config = TestConfig(test_datasource)\n",
    "\n",
    "input_map = {\n",
    "    features: train_datasource.streams.features,\n",
    "    target: train_datasource.streams.target\n",
    "}\n",
    "\n",
    "history = loss.train(\n",
    "    train_datasource, \n",
    "    epoch_size=EPOCH_SIZE,\n",
    "    parameter_learners=[learner], \n",
    "    model_inputs_to_streams=input_map,\n",
    "    callbacks=[progress_writer, test_config],\n",
    "    minibatch_size=BATCH_SIZE,\n",
    "    max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "You can use any CNTK model as a function, that's how we make our predictions in this notebook too. The model function accepts a numpy array as input. The shape of the array is defined as `<batch>x<timesteps>x<features>`. We're using a number of samples stored as a pickle file which we load and then feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8161.595],\n",
       "       [16710.596],\n",
       "       [13220.489],\n",
       "       ...,\n",
       "       [10979.5  ],\n",
       "       [15410.741],\n",
       "       [16656.523]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test_samples.pkl', 'rb') as test_file:\n",
    "    test_samples = pickle.load(test_file)\n",
    "    \n",
    "model(test_samples) * NORMALIZE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
